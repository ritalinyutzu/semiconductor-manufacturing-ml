#!/usr/bin/env python3
"""
æ¨¡å‹è©•ä¼°é¡
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import warnings

warnings.filterwarnings('ignore')


class ModelEvaluator:
    def __init__(self):
        pass
    
    def print_classification_report(self, y_test, y_pred, model_name):
        """æ‰“å°åˆ†é¡å ±å‘Š"""
        print(f"\n{model_name} - è©³ç´°åˆ†é¡å ±å‘Š")
        print("="*60)
        print(classification_report(y_test, y_pred, zero_division=0))
    
    def plot_confusion_matrix(self, y_test, y_pred, model_name, figsize=(8, 6)):
        """ç¹ªè£½æ··æ·†çŸ©é™£"""
        cm = confusion_matrix(y_test, y_pred)
        fig, ax = plt.subplots(figsize=figsize)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    xticklabels=['å¤±æ•—', 'æˆåŠŸ'], yticklabels=['å¤±æ•—', 'æˆåŠŸ'])
        ax.set_title(f'{model_name} - æ··æ·†çŸ©é™£')
        ax.set_ylabel('çœŸå¯¦')
        ax.set_xlabel('é æ¸¬')
        return fig
    
    def plot_roc_curve(self, y_test, y_proba, model_name, figsize=(8, 6)):
        """ç¹ªè£½ROCæ›²ç·š"""
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        
        fig, ax = plt.subplots(figsize=figsize)
        ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='éš¨æ©Ÿåˆ†é¡å™¨')
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('å½æ­£ç‡ (FPR)')
        ax.set_ylabel('çœŸæ­£ç‡ (TPR)')
        ax.set_title(f'{model_name} - ROCæ›²ç·š')
        ax.legend(loc="lower right")
        ax.grid(alpha=0.3)
        return fig
    
    def compare_models(self, results_df, figsize=(14, 5)):
        """å°æ¯”å¤šå€‹æ¨¡å‹"""
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        # æº–ç¢ºç‡å°æ¯”
        ax = axes[0]
        colors = ['#FF6B6B' if x != results_df['æ¸¬è©¦æº–ç¢ºç‡'].max() else '#4ECDC4' 
                  for x in results_df['æ¸¬è©¦æº–ç¢ºç‡']]
        ax.bar(results_df['æ¨¡å‹'], results_df['æ¸¬è©¦æº–ç¢ºç‡'], color=colors)
        ax.set_ylabel('æ¸¬è©¦æº–ç¢ºç‡')
        ax.set_title('æ¨¡å‹æ¸¬è©¦æº–ç¢ºç‡å°æ¯”')
        ax.set_ylim([0, 1])
        ax.grid(alpha=0.3, axis='y')
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
        
        # F1åˆ†æ•¸å°æ¯”
        ax = axes[1]
        ax.bar(results_df['æ¨¡å‹'], results_df['F1åˆ†æ•¸'], color=colors)
        ax.set_ylabel('F1åˆ†æ•¸')
        ax.set_title('æ¨¡å‹F1åˆ†æ•¸å°æ¯”')
        ax.set_ylim([0, 1])
        ax.grid(alpha=0.3, axis='y')
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
        
        return fig
    
    def get_feature_importance_from_model(self, model, feature_names):
        """å¾æ¨¡å‹ç²å–ç‰¹å¾µé‡è¦æ€§"""
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            return dict(zip(feature_names, importances))
        elif hasattr(model, 'coef_'):
            coef = np.abs(model.coef_[0])
            return dict(zip(feature_names, coef))
        return None
    
    def plot_feature_importance(self, feature_dict, top_n=20, figsize=(12, 8)):
        """ç¹ªè£½ç‰¹å¾µé‡è¦æ€§"""
        sorted_features = sorted(feature_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]
        features, importances = zip(*sorted_features)
        
        fig, ax = plt.subplots(figsize=figsize)
        ax.barh(range(len(features)), importances, color='#4ECDC4')
        ax.set_yticks(range(len(features)))
        ax.set_yticklabels(features)
        ax.set_xlabel('é‡è¦æ€§')
        ax.set_title(f'å‰{top_n}å€‹æœ€é‡è¦çš„ç‰¹å¾µ')
        ax.grid(alpha=0.3, axis='x')
        return fig
    
    def create_summary_report(self, best_model_name, y_test, y_pred, y_proba):
        """å‰µå»ºç¸½çµå ±å‘Š"""
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        report = f"""
ğŸ“Š æœ€ä½³æ¨¡å‹è©•ä¼°å ±å‘Š
{'â”€'*60}

æ¨¡å‹åç¨±: {best_model_name}

æ€§èƒ½æŒ‡æ¨™:
  - æ¸¬è©¦æº–ç¢ºç‡: {accuracy_score(y_test, y_pred):.4f}
  - ç²¾ç¢ºç‡: {precision_score(y_test, y_pred, zero_division=0):.4f}
  - å¬å›ç‡: {recall_score(y_test, y_pred, zero_division=0):.4f}
  - F1åˆ†æ•¸: {f1_score(y_test, y_pred, zero_division=0):.4f}

åˆ†é¡å ±å‘Š:
{classification_report(y_test, y_pred, zero_division=0)}
"""
        return report