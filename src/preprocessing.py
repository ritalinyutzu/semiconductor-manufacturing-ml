"""
æ•°æ®é¢„å¤„ç†æ¨¡å—
å¤„ç†ç¼ºå¤±å€¼ã€ç‰¹å¾ç¼©æ”¾ã€ç‰¹å¾é€‰æ‹©ç­‰
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings

warnings.filterwarnings('ignore')


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†ç±»"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.pca = None
        self.feature_names = None
        self.n_components = None
        
    def load_data(self, filepath):
        """
        åŠ è½½æ•°æ®é›†
        
        Args:
            filepath: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            pandas DataFrame
        """
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {filepath}")
        df = pd.read_csv(filepath)
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ! å½¢çŠ¶: {df.shape}")
        return df
    
    def check_missing_values(self, df):
        """
        æ£€æŸ¥ç¼ºå¤±å€¼
        
        Args:
            df: è¾“å…¥DataFrame
            
        Returns:
            ç¼ºå¤±å€¼ç»Ÿè®¡
        """
        print("\nğŸ“Š ç¼ºå¤±å€¼åˆ†æ:")
        missing = df.isnull().sum()
        missing_pct = (missing / len(df)) * 100
        
        missing_df = pd.DataFrame({
            'ç¼ºå¤±æ•°': missing,
            'ç¼ºå¤±ç™¾åˆ†æ¯”': missing_pct
        }).sort_values('ç¼ºå¤±æ•°', ascending=False)
        
        missing_df = missing_df[missing_df['ç¼ºå¤±æ•°'] > 0]
        if len(missing_df) == 0:
            print("âœ… æ²¡æœ‰ç¼ºå¤±å€¼ï¼")
        else:
            print(missing_df.head(10))
        
        return missing_df
    
    def handle_missing_values(self, df, strategy='drop', threshold=0.5):
        """
        å¤„ç†ç¼ºå¤±å€¼
        
        Args:
            df: è¾“å…¥DataFrame
            strategy: 'drop' æˆ– 'mean'
            threshold: ç¼ºå¤±å€¼ç™¾åˆ†æ¯”é˜ˆå€¼
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        print(f"\nğŸ”§ å¤„ç†ç¼ºå¤±å€¼ (ç­–ç•¥: {strategy}, é˜ˆå€¼: {threshold})...")
        
        # è®¡ç®—ç¼ºå¤±ç™¾åˆ†æ¯”
        missing_pct = df.isnull().sum() / len(df)
        
        # åˆ é™¤ç¼ºå¤±è¶…è¿‡é˜ˆå€¼çš„åˆ—
        cols_to_drop = missing_pct[missing_pct > threshold].index
        df = df.drop(columns=cols_to_drop)
        print(f"  - åˆ é™¤äº† {len(cols_to_drop)} åˆ—(ç¼ºå¤± > {threshold*100}%)")
        
        # å¤„ç†å‰©ä½™ç¼ºå¤±å€¼
        if strategy == 'drop':
            df = df.dropna()
            print(f"  - åˆ é™¤äº†å«æœ‰ç¼ºå¤±å€¼çš„è¡Œ")
        elif strategy == 'mean':
            df = df.fillna(df.mean())
            print(f"  - ç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼")
        
        print(f"âœ… å¤„ç†åå½¢çŠ¶: {df.shape}")
        return df
    
    def remove_zero_variance_features(self, df, exclude_cols=None):
        """
        ç§»é™¤é›¶æ–¹å·®ç‰¹å¾ï¼ˆæ— ä¿¡æ¯çš„åˆ—ï¼‰
        
        Args:
            df: è¾“å…¥DataFrame
            exclude_cols: éœ€è¦æ’é™¤çš„åˆ—ï¼ˆå¦‚ç›®æ ‡åˆ—ï¼‰
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        print("\nğŸ—‘ï¸ ç§»é™¤é›¶æ–¹å·®ç‰¹å¾...")
        exclude_cols = exclude_cols or []
        variance = df.drop(columns=exclude_cols).select_dtypes(include=[np.number]).var()
        zero_var_features = variance[variance == 0].index.tolist()
        
        if zero_var_features:
            df = df.drop(columns=zero_var_features)
            print(f"  - åˆ é™¤äº† {len(zero_var_features)} ä¸ªé›¶æ–¹å·®ç‰¹å¾")
        else:
            print("  - æ²¡æœ‰é›¶æ–¹å·®ç‰¹å¾")
        
        print(f"âœ… å¤„ç†åå½¢çŠ¶: {df.shape}")
        return df
    
    def scale_features(self, X_train, X_test=None, fit=True):
        """
        ç‰¹å¾ç¼©æ”¾
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            X_test: æµ‹è¯•é›†ç‰¹å¾
            fit: æ˜¯å¦æ‹Ÿåˆscaler
            
        Returns:
            ç¼©æ”¾åçš„ç‰¹å¾
        """
        print("\nğŸ“ˆ ç‰¹å¾ç¼©æ”¾ (StandardScaler)...")
        
        if fit:
            X_train_scaled = self.scaler.fit_transform(X_train)
        else:
            X_train_scaled = self.scaler.transform(X_train)
        
        print(f"âœ… è®­ç»ƒé›†ç¼©æ”¾å®Œæˆ")
        
        if X_test is not None:
            X_test_scaled = self.scaler.transform(X_test)
            print(f"âœ… æµ‹è¯•é›†ç¼©æ”¾å®Œæˆ")
            return X_train_scaled, X_test_scaled
        
        return X_train_scaled
    
    def apply_pca(self, X_train, X_test=None, variance_ratio=0.95, fit=True):
        """
        åº”ç”¨PCAé™ç»´
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            X_test: æµ‹è¯•é›†ç‰¹å¾
            variance_ratio: ä¿ç•™çš„æ–¹å·®æ¯”ä¾‹
            fit: æ˜¯å¦æ‹ŸåˆPCA
            
        Returns:
            é™ç»´åçš„ç‰¹å¾
        """
        if fit:
            print(f"\nğŸ“‰ PCAé™ç»´ (ä¿ç•™æ–¹å·®: {variance_ratio*100}%)...")
            
            # å…ˆç”¨æ‰€æœ‰ç‰¹å¾æ‹Ÿåˆä»¥ç¡®å®šæœ€ä¼˜ç»„ä»¶æ•°
            pca_temp = PCA()
            pca_temp.fit(X_train)
            
            # è®¡ç®—éœ€è¦çš„ç»„ä»¶æ•°
            cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
            n_components = np.argmax(cumsum >= variance_ratio) + 1
            
            # åˆ›å»ºæœ€ç»ˆçš„PCAå¯¹è±¡
            self.pca = PCA(n_components=n_components)
            X_train_pca = self.pca.fit_transform(X_train)
            
            self.n_components = n_components
            variance_explained = self.pca.explained_variance_ratio_.sum()
            
            print(f"  - åŸå§‹ç‰¹å¾æ•°: {X_train.shape[1]}")
            print(f"  - é™ç»´åç‰¹å¾æ•°: {n_components}")
            print(f"  - è§£é‡Šæ–¹å·®æ¯”: {variance_explained:.2%}")
            print(f"âœ… PCAæ‹Ÿåˆå®Œæˆ")
        else:
            X_train_pca = self.pca.transform(X_train)
        
        if X_test is not None:
            X_test_pca = self.pca.transform(X_test)
            return X_train_pca, X_test_pca
        
        return X_train_pca
    
    def get_feature_importance_pca(self):
        """
        è·å–PCAç‰¹å¾é‡è¦æ€§
        
        Returns:
            ç‰¹å¾é‡è¦æ€§DataFrame
        """
        if self.pca is None:
            print("âŒ PCAè¿˜æœªæ‹Ÿåˆ!")
            return None
        
        loadings = self.pca.components_.T * np.sqrt(self.pca.explained_variance_)
        importance = np.abs(loadings).mean(axis=1)
        
        importance_df = pd.DataFrame({
            'ç‰¹å¾': [f'PC{i+1}' for i in range(self.n_components)],
            'é‡è¦æ€§': importance
        }).sort_values('é‡è¦æ€§', ascending=False)
        
        return importance_df


class FeatureSelector:
    """ç‰¹å¾é€‰æ‹©ç±»"""
    
    @staticmethod
    def select_by_variance_threshold(X, threshold=0.01):
        """
        é€šè¿‡æ–¹å·®é˜ˆå€¼é€‰æ‹©ç‰¹å¾
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            threshold: æ–¹å·®é˜ˆå€¼
            
        Returns:
            é€‰ä¸­çš„ç‰¹å¾åˆ—è¡¨
        """
        from sklearn.feature_selection import VarianceThreshold
        
        selector = VarianceThreshold(threshold=threshold)
        selector.fit(X)
        
        selected_features = X.columns[selector.get_support()].tolist()
        print(f"âœ… é€šè¿‡æ–¹å·®é˜ˆå€¼ä¿ç•™ {len(selected_features)}/{len(X.columns)} ç‰¹å¾")
        
        return selected_features
    
    @staticmethod
    def select_by_correlation(X, target=None, threshold=0.9):
        """
        é€šè¿‡ç›¸å…³æ€§é˜ˆå€¼ç§»é™¤å¤šé‡å…±çº¿æ€§ç‰¹å¾
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            target: ç›®æ ‡åˆ—
            threshold: ç›¸å…³æ€§é˜ˆå€¼
            
        Returns:
            å¤„ç†åçš„ç‰¹å¾çŸ©é˜µ
        """
        corr_matrix = X.select_dtypes(include=[np.number]).corr().abs()
        upper = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
        X_selected = X.drop(columns=to_drop)
        
        print(f"âœ… ç§»é™¤ {len(to_drop)} ä¸ªé«˜åº¦ç›¸å…³çš„ç‰¹å¾")
        return X_selected

