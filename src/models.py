#!/usr/bin/env python3
"""
æ¨¡å‹è¨“ç·´å’Œè©•ä¼°é¡
"""

import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import warnings

warnings.filterwarnings('ignore')


class ModelTrainer:
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.best_model = None
        self.best_model_name = None
        self.trained_models = {}
        self.results = None
    
    def train_knn(self, X_train, y_train):
        """è¨“ç·´KNNæ¨¡å‹"""
        model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
        X_train = np.ascontiguousarray(X_train)
        model.fit(X_train, y_train)
        return model
    
    def train_naive_bayes(self, X_train, y_train):
        """è¨“ç·´é«˜æ–¯æ¨¸ç´ è²è‘‰æ–¯"""
        model = GaussianNB()
        model.fit(X_train, y_train)
        return model
    
    def train_logistic_regression(self, X_train, y_train):
        """è¨“ç·´é‚è¼¯è¿´æ­¸"""
        model = LogisticRegression(random_state=self.random_state, max_iter=1000)
        model.fit(X_train, y_train)
        return model
    
    def train_random_forest(self, X_train, y_train):
        """è¨“ç·´éš¨æ©Ÿæ£®æ—"""
        model = RandomForestClassifier(
            n_estimators=100, 
            random_state=self.random_state,
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        return model
    
    def train_xgboost(self, X_train, y_train):
        """è¨“ç·´XGBoost"""
        model = XGBClassifier(
            n_estimators=100,
            random_state=self.random_state,
            use_label_encoder=False,
            eval_metric='logloss',
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        return model
    
    def evaluate_model(self, model, X_train, X_test, y_train, y_test, model_name):
        """è©•ä¼°å–®å€‹æ¨¡å‹"""
        # ç¢ºä¿æ•¸æ“šæ ¼å¼æ­£ç¢º
        X_train = np.ascontiguousarray(X_train)
        X_test = np.ascontiguousarray(X_test)
        
        # è¨“ç·´é›†é æ¸¬
        y_train_pred = model.predict(X_train)
        train_acc = accuracy_score(y_train, y_train_pred)
        
        # æ¸¬è©¦é›†é æ¸¬
        y_test_pred = model.predict(X_test)
        test_acc = accuracy_score(y_test, y_test_pred)
        
        # å…¶ä»–æŒ‡æ¨™
        precision = precision_score(y_test, y_test_pred, zero_division=0)
        recall = recall_score(y_test, y_test_pred, zero_division=0)
        f1 = f1_score(y_test, y_test_pred, zero_division=0)
        
        # äº¤å‰é©—è­‰
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        
        # ROC-AUC
        try:
            if hasattr(model, 'predict_proba'):
                y_proba = model.predict_proba(X_test)[:, 1]
                roc_auc = roc_auc_score(y_test, y_proba)
            else:
                roc_auc = 0.0
        except:
            roc_auc = 0.0
        
        return {
            'æ¨¡å‹': model_name,
            'è¨“ç·´æº–ç¢ºç‡': train_acc,
            'æ¸¬è©¦æº–ç¢ºç‡': test_acc,
            'ç²¾ç¢ºç‡': precision,
            'å¬å›ç‡': recall,
            'F1åˆ†æ•¸': f1,
            'ROC-AUC': roc_auc,
            'äº¤å‰é©—è­‰å¹³å‡': cv_scores.mean(),
            'äº¤å‰é©—è­‰æ¨™æº–å·®': cv_scores.std()
        }
    
    def train_and_evaluate_all(self, X_train, y_train, X_test, y_test):
        """è¨“ç·´å’Œè©•ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸ¤– æ¨¡å‹è¨“ç·´å’Œè©•ä¼°")
        print("="*60 + "\n")
        
        models_config = [
            ('KNN', self.train_knn),
            ('é«˜æ–¯æ¨¸ç´ è²è‘‰æ–¯', self.train_naive_bayes),
            ('é‚è¼¯è¿´æ­¸', self.train_logistic_regression),
            ('éš¨æ©Ÿæ£®æ—', self.train_random_forest),
            ('XGBoost', self.train_xgboost)
        ]
        
        results = []
        
        for model_name, train_func in models_config:
            print(f"ğŸš€ è¨“ç·´ {model_name}...")
            try:
                model = train_func(X_train, y_train)
                self.trained_models[model_name] = model
                
                result = self.evaluate_model(model, X_train, X_test, y_train, y_test, model_name)
                results.append(result)
                
                print(f"âœ… {model_name} è¨“ç·´å®Œæˆ")
                print(f"   æ¸¬è©¦æº–ç¢ºç‡: {result['æ¸¬è©¦æº–ç¢ºç‡']:.4f}")
                print()
            except Exception as e:
                print(f"âŒ {model_name} è¨“ç·´å¤±æ•—: {str(e)}")
                print()
        
        self.results = pd.DataFrame(results)
        
        # é¸æ“‡æœ€ä½³æ¨¡å‹
        best_idx = self.results['æ¸¬è©¦æº–ç¢ºç‡'].idxmax()
        self.best_model_name = self.results.loc[best_idx, 'æ¨¡å‹']
        self.best_model = self.trained_models[self.best_model_name]
        
        print("="*60)
        print(f"âœ… æœ€ä½³æ¨¡å‹: {self.best_model_name}")
        print(f"   æ¸¬è©¦æº–ç¢ºç‡: {self.results.loc[best_idx, 'æ¸¬è©¦æº–ç¢ºç‡']:.4f}")
        print("="*60 + "\n")
        
        return self.results, self.trained_models
    
    def get_best_model(self):
        """ç²å–æœ€ä½³æ¨¡å‹"""
        return self.best_model_name, self.best_model
    
    def get_prediction_probabilities(self, model_name, X_test):
        """ç²å–é æ¸¬æ¦‚ç‡"""
        model = self.trained_models[model_name]
        if hasattr(model, 'predict_proba'):
            return model.predict_proba(X_test)[:, 1]
        return None