#!/usr/bin/env python3
"""
ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹è¨“ç·´å’Œè©•ä¼°
é‹è¡Œ: python scripts/03_modeling.py
"""

import sys
sys.path.append('..')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from src.models import ModelTrainer
from src.evaluate import ModelEvaluator
from src.utils import print_header, print_section
from sklearn.metrics import confusion_matrix
import warnings

warnings.filterwarnings('ignore')


def main():
    print_header("ğŸ¤– ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹è¨“ç·´å’Œè©•ä¼°")
    
    # æ­¥é©Ÿ1: è¼‰å…¥é è™•ç†å¾Œçš„æ•¸æ“š
    print_section("æ­¥é©Ÿ1: è¼‰å…¥é è™•ç†å¾Œçš„æ•¸æ“š")
    
    X_train = pd.read_csv('data/processed/X_train_pca.csv')
    X_test = pd.read_csv('data/processed/X_test_pca.csv')
    y_train = pd.read_csv('data/processed/y_train.csv').iloc[:, 0]
    y_test = pd.read_csv('data/processed/y_test.csv').iloc[:, 0]
    
    print(f"âœ… æ•¸æ“šè¼‰å…¥å®Œæˆ")
    print(f"   X_train: {X_train.shape}")
    print(f"   X_test: {X_test.shape}")
    print(f"   y_train: {y_train.shape}")
    print(f"   y_test: {y_test.shape}")
    
    # æ­¥é©Ÿ2: é©—è­‰æ•¸æ“š
    print_section("æ­¥é©Ÿ2: é©—è­‰æ•¸æ“š")
    print(f"è¨“ç·´é›†ç›®æ¨™åˆ†å¸ƒ: {dict(y_train.value_counts().sort_index())}")
    print(f"æ¸¬è©¦é›†ç›®æ¨™åˆ†å¸ƒ: {dict(y_test.value_counts().sort_index())}")
    print(f"ç‰¹å¾µç¶­åº¦: {X_train.shape[1]}")
    
    # æ­¥é©Ÿ3: è½‰æ›æ¨™ç±¤ï¼ˆ[-1, 1] æ”¹ç‚º [0, 1]ï¼‰
    print_section("æ­¥é©Ÿ3: æ¨™ç±¤è½‰æ›")
    y_train = (y_train + 1) // 2  # -1 -> 0, 1 -> 1
    y_test = (y_test + 1) // 2
    print(f"âœ… æ¨™ç±¤å·²è½‰æ›")
    print(f"   è¨“ç·´é›†: {dict(y_train.value_counts().sort_index())}")
    print(f"   æ¸¬è©¦é›†: {dict(y_test.value_counts().sort_index())}")
    
    # è½‰æ›ç‚ºnumpyæ•¸çµ„ä¸¦ç¢ºä¿æ˜¯é€£çºŒçš„
    X_train = np.ascontiguousarray(X_train.values, dtype=np.float64)
    X_test = np.ascontiguousarray(X_test.values, dtype=np.float64)
    y_train = y_train.values
    y_test = y_test.values
    
    # æ­¥é©Ÿ4: è¨“ç·´æ‰€æœ‰æ¨¡å‹
    print_section("æ­¥é©Ÿ4: è¨“ç·´æ‰€æœ‰æ¨¡å‹")
    
    trainer = ModelTrainer(random_state=42)
    results_df, trained_models = trainer.train_and_evaluate_all(
        X_train, y_train, X_test, y_test
    )
    
    # æ­¥é©Ÿ5: å¯è¦–åŒ–æ¨¡å‹å°æ¯”
    print_section("æ­¥é©Ÿ5: æ¨¡å‹æ€§èƒ½å°æ¯”")
    
    evaluator = ModelEvaluator()
    fig = evaluator.compare_models(results_df, figsize=(14, 5))
    plt.savefig('results/figures/04_model_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ… å°æ¯”åœ–å·²ä¿å­˜: results/figures/04_model_comparison.png")
    plt.close()
    
    # æ­¥é©Ÿ6: ç²å–æœ€ä½³æ¨¡å‹
    print_section("æ­¥é©Ÿ6: é¸æ“‡æœ€ä½³æ¨¡å‹")
    
    best_model_name, best_model = trainer.get_best_model()
    
    # è½‰æ›ç‚ºé€£çºŒæ•¸çµ„é€²è¡Œé æ¸¬
    X_test_cont = np.ascontiguousarray(X_test, dtype=np.float64)
    y_pred = best_model.predict(X_test_cont)
    y_proba = trainer.get_prediction_probabilities(best_model_name, X_test_cont)
    
    print(f"âœ… æœ€ä½³æ¨¡å‹: {best_model_name}")
    
    # æ­¥é©Ÿ7: æœ€ä½³æ¨¡å‹çš„è©³ç´°è©•ä¼°
    print_section("æ­¥é©Ÿ7: æœ€ä½³æ¨¡å‹è©³ç´°è©•ä¼°")
    
    evaluator.print_classification_report(y_test, y_pred, best_model_name)
    
    # æ­¥é©Ÿ8: æ··æ·†çŸ©é™£
    print_section("æ­¥é©Ÿ8: æ··æ·†çŸ©é™£")
    
    fig = evaluator.plot_confusion_matrix(y_test, y_pred, best_model_name, figsize=(8, 6))
    plt.savefig(f'results/figures/05_confusion_matrix_{best_model_name}.png', 
                dpi=300, bbox_inches='tight')
    print(f"âœ… æ··æ·†çŸ©é™£å·²ä¿å­˜")
    plt.close()
    
    # æ­¥é©Ÿ9: ROCæ›²ç·š
    print_section("æ­¥é©Ÿ9: ROCæ›²ç·š")
    
    if y_proba is not None:
        fig = evaluator.plot_roc_curve(y_test, y_proba, best_model_name, figsize=(8, 6))
        plt.savefig(f'results/figures/06_roc_curve_{best_model_name}.png', 
                    dpi=300, bbox_inches='tight')
        print(f"âœ… ROCæ›²ç·šå·²ä¿å­˜")
        plt.close()
    else:
        print("âš ï¸ æœ€ä½³æ¨¡å‹ä¸æ”¯æŒæ¦‚ç‡é æ¸¬")
    
    # æ­¥é©Ÿ10: ç‰¹å¾µé‡è¦æ€§
    print_section("æ­¥é©Ÿ10: ç‰¹å¾µé‡è¦æ€§åˆ†æ")
    
    feature_names = [f'PC{i+1}' for i in range(X_train.shape[1])]
    feature_importance = evaluator.get_feature_importance_from_model(
        best_model, feature_names
    )
    
    if feature_importance:
        fig = evaluator.plot_feature_importance(feature_importance, figsize=(12, 8))
        plt.savefig(f'results/figures/07_feature_importance_{best_model_name}.png', 
                    dpi=300, bbox_inches='tight')
        print(f"âœ… ç‰¹å¾µé‡è¦æ€§åœ–å·²ä¿å­˜")
        plt.close()
    else:
        print("âš ï¸ æ¨¡å‹ä¸æ”¯æŒç‰¹å¾µé‡è¦æ€§æå–")
    
    # æ­¥é©Ÿ11: æ‰€æœ‰æ¨¡å‹çš„æ··æ·†çŸ©é™£
    print_section("æ­¥é©Ÿ11: æ‰€æœ‰æ¨¡å‹çš„æ··æ·†çŸ©é™£å°æ¯”")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    model_names = list(trained_models.keys())
    for idx, model_name in enumerate(model_names[:4]):
        model = trained_models[model_name]
        y_pred_temp = model.predict(X_test_cont)
        cm = confusion_matrix(y_test, y_pred_temp)
        
        ax = axes[idx]
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    xticklabels=['å¤±æ•—', 'æˆåŠŸ'], yticklabels=['å¤±æ•—', 'æˆåŠŸ'])
        ax.set_title(f'{model_name}')
        ax.set_ylabel('çœŸå¯¦')
        ax.set_xlabel('é æ¸¬')
    
    plt.tight_layout()
    plt.savefig('results/figures/08_all_confusion_matrices.png', dpi=300, bbox_inches='tight')
    print("âœ… æ‰€æœ‰æ··æ·†çŸ©é™£å·²ä¿å­˜")
    plt.close()
    
    # æ­¥é©Ÿ12: ä¿å­˜æœ€ä½³æ¨¡å‹
    print_section("æ­¥é©Ÿ12: ä¿å­˜æœ€ä½³æ¨¡å‹")
    
    model_path = f'results/models/best_model_{best_model_name}.pkl'
    joblib.dump(best_model, model_path)
    print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # ä¿å­˜æ‰€æœ‰æ¨¡å‹
    for model_name, model in trained_models.items():
        path = f'results/models/model_{model_name.replace(" ", "_")}.pkl'
        joblib.dump(model, path)
        print(f"âœ… {model_name} å·²ä¿å­˜")
    
    # æ­¥é©Ÿ13: ç”Ÿæˆæœ€çµ‚ç¸½çµå ±å‘Š
    print_section("æ­¥é©Ÿ13: æœ€çµ‚ç¸½çµå ±å‘Š")
    
    summary_report = evaluator.create_summary_report(
        best_model_name, y_test, y_pred, y_proba
    )
    print(summary_report)
    
    # ä¿å­˜å ±å‘Š
    with open('results/modeling_summary.txt', 'w', encoding='utf-8') as f:
        f.write("\n" + results_df.to_string(index=False) + "\n")
        f.write(summary_report)
    
    print("âœ… ç¸½çµå ±å‘Šå·²ä¿å­˜: results/modeling_summary.txt")
    
    # æ­¥é©Ÿ14: æ¨¡å‹è³‡è¨Šç¸½çµ
    print_section("ğŸ‰ é …ç›®å®Œæˆç¸½çµ")
    
    final_summary = f"""
ğŸ“Š åŠå°é«”è£½é€ ç¼ºé™·é æ¸¬ - æœ€çµ‚å ±å‘Š
{'â”€'*60}

âœ… é …ç›®å®Œæˆï¼

æœ€ä½³æ¨¡å‹: {best_model_name}
  - æ¸¬è©¦æº–ç¢ºç‡: {results_df[results_df['æ¨¡å‹']==best_model_name]['æ¸¬è©¦æº–ç¢ºç‡'].values[0]:.4f}

æ¨¡å‹æ€§èƒ½æ’å:
"""
    
    for idx, row in results_df.sort_values('æ¸¬è©¦æº–ç¢ºç‡', ascending=False).iterrows():
        final_summary += f"  {idx+1}. {row['æ¨¡å‹']}: {row['æ¸¬è©¦æº–ç¢ºç‡']:.4f}\n"
    
    final_summary += f"""
å·²ç”Ÿæˆæª”æ¡ˆ:
  âœ… models/ - è¨“ç·´å¥½çš„æ¨¡å‹ (.pkl)
  âœ… figures/ - å¯è¦–åŒ–åœ–è¡¨
  âœ… modeling_summary.txt - è©³ç´°å ±å‘Š

ğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆï¼
"""
    
    print(final_summary)
    
    with open('results/figures/final_summary.txt', 'w', encoding='utf-8') as f:
        f.write(final_summary)
    
    print("âœ… æœ€çµ‚ç¸½çµå·²ä¿å­˜: results/figures/final_summary.txt")
    
    print("\n" + "="*60)
    print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼")
    print("="*60)
    print("\nğŸš€ ä¸‹ä¸€æ­¥: python scripts/04_results.py")


if __name__ == '__main__':
    main()